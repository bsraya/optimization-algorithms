# Optimization Algorithms

<<<<<<< HEAD
This repository contains implementations of various optimization algorithms that you might have encountered while using [Keras](https://keras.io/api/optimizers/) or (Pytorch)[https://pytorch.org/docs/stable/optim.html]. 

It is an attempt of writing the algorithms in a way that is easy to understand and use from scratch.

Here is the progress of the project:

* :white_check_mark: Batch Gradient Descent 
* :white_check_mark: Mini-Batch Gradient Descent
* :white_check_mark: Stochastic Gradient Descent
* [ ] SGD with Momentum 
* [ ] SGD with weight decay
* [ ] SGD with Momentum, Nesterov, and weight decay
=======
1. Batch Gradient Descent :white_check_mark:
2. Mini-Batch Gradient Descent :white_check_mark:
3. Stochastic Gradient Descent :white_check_mark:
4. SGD with Momentum (on-going)
>>>>>>> 1f73414befa357d916bad7b40768f8a85e3c1a8e
